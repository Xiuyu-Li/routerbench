"""
this router does not follow the abstract_router class.
It takes in "router_inputs" file generated by clean_embed_data.py and generate eval_results file
"""

import numpy as np
import pandas as pd
import tqdm
from tqdm.auto import tqdm as tqdm_auto

from routers.abstract_router import (get_completion_token_cost,
                                     get_model_request_cost,
                                     get_prompt_token_cost,
                                     get_tokens_for_response)
from utils import generate_datetime_str

tqdm_auto.pandas()

MAX_COST_PER_RESPONSE = 100


def run_cascading_router_for_eval(
    inputs_df: pd.DataFrame,
    performance_threshold_quantile: float,
    max_cost_per_response: float,
    evaluator_error_rate: float,
    eval_name: str,
    models_to_route: list[str] = (" ",),
) -> dict:
    assert inputs_df.eval_name.nunique() == 1
    performance_threshold = np.quantile(
        inputs_df[models_to_route].values.flatten(), q=performance_threshold_quantile
    )

    average_response_length = {}
    for model in models_to_route:
        average_response_length[model] = (
            inputs_df[f"{model}|model_response"]
            .apply(lambda x: get_tokens_for_response(x, model))
            .mean()
        )

    # generate a list of models based on the cost, ranked from lower to higher
    model_completion_cost = {
        key: get_completion_token_cost(key)
        + get_prompt_token_cost(key)
        + (get_model_request_cost(key) / average_response_length[key])
        for key in models_to_route
    }
    llms = [i for i in list(model_completion_cost.keys()) if i != "no_model_correct"]
    llms.sort(key=lambda x: model_completion_cost[x])

    eval_set_total_performance_score = 0
    total_cost = 0
    num_sample = 0

    for idx, row in inputs_df.iterrows():
        num_sample += 1
        curr_max_cost_per_response = max_cost_per_response
        sample_cost = 0

        for llm_name in llms:
            if (
                row[llm_name + "|total_cost"] >= curr_max_cost_per_response
                and sample_cost > 0
            ):
                break  # Stop if max cost is exceeded

            # Introduce evaluator error
            evaluator_wrong = np.random.rand() < evaluator_error_rate
            sample_cost += row[llm_name + "|total_cost"]
            curr_max_cost_per_response -= row[llm_name + "|total_cost"]

            if evaluator_wrong:
                answer_accepted = row[llm_name] < performance_threshold
            else:
                answer_accepted = row[llm_name] >= performance_threshold

            last_performance = row[llm_name]

            if answer_accepted:
                break

        eval_set_total_performance_score += last_performance
        total_cost += sample_cost

    # Calculate overall performance
    avg_performance = eval_set_total_performance_score / num_sample

    output_dict = {
        "model_name": "cascading router",
        "eval_name": eval_name,
        "performance": avg_performance,
        "total_cost": total_cost,
        "max_cost_per_response": max_cost_per_response,
        "evaluator_error_rate": evaluator_error_rate,
        "performance_threshold": performance_threshold,
    }

    return output_dict


def run_cascading_router_for_eval_dataframe(
    eval_df: pd.DataFrame,
    eval_name: str,
    max_cost_per_response_list: list[float],
    error_rates: list[float],
    models_to_route: list[str] = (" ",),
) -> str:
    print(f"Running cascading router for {eval_name}")
    original_row_n = eval_df.shape[0]
    eval_df = eval_df.dropna()
    output_df = []
    if eval_df.shape[0] < original_row_n:
        print(f"Dropped {original_row_n - eval_df.shape[0]} rows due to NaN values")
    for e_name in eval_df[
        eval_df.eval_name.str.startswith(eval_name)
    ].eval_name.unique():
        for max_cost_per_response, evaluator_error_rate in tqdm.tqdm(
            [
                (cost, rate)
                for cost in max_cost_per_response_list
                for rate in error_rates
            ]
        ):
            output_dict = run_cascading_router_for_eval(
                inputs_df=eval_df[eval_df.eval_name == e_name],
                max_cost_per_response=max_cost_per_response,
                evaluator_error_rate=evaluator_error_rate,
                performance_threshold_quantile=0.95,  # top 5% performance
                eval_name=e_name,
                models_to_route=models_to_route,
            )
            output_df.append(output_dict)

    output_df = pd.DataFrame(output_df)

    csv_filename = f"data/eval_results/{generate_datetime_str(minute=True)}-cascading_router_{eval_name}_results.csv"
    output_df.to_csv(csv_filename, index=False)
    print(f"Saved results to {csv_filename}")
    return csv_filename
